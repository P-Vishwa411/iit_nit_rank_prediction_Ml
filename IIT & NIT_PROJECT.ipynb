{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1915beb7",
   "metadata": {},
   "source": [
    "###  step1:-  Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Ignore harmless warnings \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set to display all the columns in dataset\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Import psql to run queries \n",
    "\n",
    "import pandasql as psql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e878e",
   "metadata": {},
   "source": [
    "### step:- Load data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b753424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load \n",
    "rankdt=pd.read_csv(r\"data.csv\",header=0)\n",
    "\n",
    "rankdt_BK=rankdt.copy()\n",
    "\n",
    "rankdt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ea3ed",
   "metadata": {},
   "source": [
    "### step2:- Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc133b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run info()\n",
    "rankdt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eaf74a",
   "metadata": {},
   "source": [
    "### step3:- check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515581fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading null values\n",
    "rankdt.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a03cd7",
   "metadata": {},
   "source": [
    "### step4:- Unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7266ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankdt.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e698bb",
   "metadata": {},
   "source": [
    "### step5:- Read Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d2b32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rankdt.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036fa7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['id', 'year', 'institute_type', 'round_no', 'quota', 'pool',\n",
    "       'institute_short', 'program_name', 'program_duration', 'degree_short',\n",
    "       'category', 'opening_rank', 'closing_rank', 'is_preparatory']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabe30f",
   "metadata": {},
   "source": [
    "### step6:- Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe80b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankdt_dup=rankdt[rankdt.duplicated(keep='last')]\n",
    "rankdt_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2df4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankdt.drop_duplicates(inplace=True)\n",
    "rankdt.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankdt[\"degree_short\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankdt[\"institute_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankdt[\"quota\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4357c72",
   "metadata": {},
   "source": [
    "### Lableencoder for target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c424663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lableencoder for target variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LE=LabelEncoder()\n",
    "rankdt['institute_type']=LE.fit_transform(rankdt['institute_type'])\n",
    "rankdt['quota']=LE.fit_transform(rankdt['quota'])\n",
    "rankdt['pool']=LE.fit_transform(rankdt['pool'])\n",
    "rankdt['institute_short']=LE.fit_transform(rankdt['institute_short'])\n",
    "rankdt['program_name']=LE.fit_transform(rankdt['program_name'])\n",
    "rankdt['program_duration']=LE.fit_transform(rankdt['program_duration'])\n",
    "rankdt['degree_short']=LE.fit_transform(rankdt['degree_short'])\n",
    "rankdt['category']=LE.fit_transform(rankdt['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9625eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target\n",
    "\n",
    "closing_rank_count=rankdt.closing_rank.value_counts()\n",
    "print('Class 0:',closing_rank_count[0])\n",
    "print('Class 1:',closing_rank_count[1])\n",
    "print('Proportion:',round(closing_rank_count[0] /closing_rank_count[1] ,2) , ':1')\n",
    "print('Total Nit records:',len(rankdt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa24afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankdt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankdt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddacb99",
   "metadata": {},
   "source": [
    "### step7:-Identify the independent and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e169397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the independent and target variable\n",
    "\n",
    "IndepVar=[]\n",
    "for col in rankdt.columns:\n",
    "    if col!='is_preparatory':\n",
    "        IndepVar.append(col)\n",
    "        \n",
    "        \n",
    "TargetVar='is_preparatory'\n",
    "\n",
    "x=rankdt[IndepVar]\n",
    "y=rankdt[TargetVar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imblearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b01172",
   "metadata": {},
   "source": [
    "### Oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7316ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random oversampling can be implemented using the RandomOverSampler class\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy=0.125)\n",
    "\n",
    "x_over, y_over = oversample.fit_resample(x, y)\n",
    "\n",
    "print(x_over.shape)\n",
    "print(y_over.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999435d",
   "metadata": {},
   "source": [
    "### step7:-Split the data into train and test(random sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaaeeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_over, y_over, test_size = 0.30, random_state = 42)\n",
    "\n",
    "# Display the shape \n",
    "\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6264544",
   "metadata": {},
   "source": [
    "### step8:-Scaling the features by using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122edd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the features by using MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmscaler=MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "x_train=mmscaler.fit_transform(x_train)\n",
    "#x_train[cols]=mmscaler.fit_transform(x_train[cols])\n",
    "x_train=pd.DataFrame(x_train)\n",
    "\n",
    "x_test=mmscaler.fit_transform(x_test)\n",
    "#x_test[cols]=mmscaler.fit_transform(x_test[cols])\n",
    "x_test=pd.DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a7f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMResults=pd.read_csv(r\"EMResults.csv\",header=0)\n",
    "EMResults.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bd40a9",
   "metadata": {},
   "source": [
    "### Compare Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64acbf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==Step_02=============================================================================================================>\n",
    "\n",
    "# Build the Calssification models and compare the results\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create objects of classification algorithms with default hyper-parameters\n",
    "\n",
    "ModelLR = LogisticRegression()\n",
    "ModelDC = DecisionTreeClassifier()\n",
    "ModelRF = RandomForestClassifier()\n",
    "ModelET = ExtraTreesClassifier()\n",
    "ModelKNN = KNeighborsClassifier(n_neighbors=5)\n",
    "ModelGNB = GaussianNB()\n",
    "ModelSVM = SVC(probability=True)\n",
    "\n",
    "# Evalution matrix for all the algorithms\n",
    "\n",
    "#MM = [ModelLR, ModelDC, ModelRF, ModelET, ModelKNN, ModelGNB, ModelSVM]\n",
    "MM = [ModelLR, ModelDC, ModelRF, ModelET,ModelKNN, ModelGNB, ModelSVM]\n",
    "for models in MM:\n",
    "            \n",
    "    # Train the model training dataset\n",
    "    \n",
    "    models.fit(x_train, y_train)\n",
    "    \n",
    "    # Prediction the model with test dataset\n",
    "    \n",
    "    y_pred = models.predict(x_test)\n",
    "    y_pred_prob = models.predict_proba(x_test)\n",
    "    \n",
    "    # Print the model name\n",
    "    \n",
    "    print('Model Name: ', models)\n",
    "    \n",
    "    # confusion matrix in sklearn\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # actual values\n",
    "\n",
    "    actual = y_test\n",
    "\n",
    "    # predicted values\n",
    "\n",
    "    predicted = y_pred\n",
    "\n",
    "    # confusion matrix\n",
    "\n",
    "    matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "    print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "    # outcome values order in sklearn\n",
    "\n",
    "    tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "    print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "    # classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "    C_Report = classification_report(actual,predicted,labels=[1,0])\n",
    "\n",
    "    print('Classification report : \\n', C_Report)\n",
    "\n",
    "    # calculating the metrics\n",
    "\n",
    "    sensitivity = round(tp/(tp+fn), 3);\n",
    "    specificity = round(tn/(tn+fp), 3);\n",
    "    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "    precision = round(tp/(tp+fp), 3);\n",
    "    f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "    # Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "    # A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "    from math import sqrt\n",
    "\n",
    "    mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "\n",
    "    print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "    print('Precision :', round(precision*100, 2),'%')\n",
    "    print('Recall :', round(sensitivity*100,2), '%')\n",
    "    print('F1 Score :', f1Score)\n",
    "    print('Specificity or True Negative Rate :', round(specificity*100,2), '%'  )\n",
    "    print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "    print('MCC :', MCC)\n",
    "\n",
    "    # Area under ROC curve \n",
    "\n",
    "    from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "    print('roc_auc_score:', round(roc_auc_score(actual, predicted), 3))\n",
    "    \n",
    "    # ROC Curve\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    Model_roc_auc = roc_auc_score(actual, predicted)\n",
    "    fpr, tpr, thresholds = roc_curve(actual, models.predict_proba(x_test)[:,1])\n",
    "    plt.figure()\n",
    "    #\n",
    "    plt.plot(fpr, tpr, label= 'Classification Model' % Model_roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('Log_ROC')\n",
    "    plt.show()\n",
    "    print('-----------------------------------------------------------------------------------------------------')\n",
    "    #----------------------------------------------------------------------------------------------------------\n",
    "    new_row = {'Model Name' : models,\n",
    "               'True_Positive': tp,\n",
    "               'False_Negative': fn, \n",
    "               'False_Positive': fp, \n",
    "               'True_Negative': tn,\n",
    "               'Accuracy' : accuracy,\n",
    "               'Precision' : precision,\n",
    "               'Recall' : sensitivity,\n",
    "               'F1 Score' : f1Score,\n",
    "               'Specificity' : specificity,\n",
    "               'MCC':MCC,\n",
    "               'ROC_AUC_Score':roc_auc_score(actual, predicted),\n",
    "               'Balanced Accuracy':balanced_accuracy}\n",
    "    EMResults = EMResults.append(new_row, ignore_index=True)\n",
    "    #----------------------------------------------------------------------------------------------------------\n",
    "#======================================================================================================================>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e89ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMResults\n",
    "EMResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e63a61",
   "metadata": {},
   "source": [
    "### Predict result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predRF=ModelET.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results=pd.DataFrame({'is_preparatory_A':y_test,'is_preparatory_P':y_predRF})\n",
    "\n",
    "ResultsFinal=rankdt_BK.merge(Results,left_index=True,right_index=True)\n",
    "\n",
    "ResultsFinal.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9489696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5e026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
